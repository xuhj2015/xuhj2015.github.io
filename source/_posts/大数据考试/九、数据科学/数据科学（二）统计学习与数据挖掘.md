---
title: 数据科学（二）统计学习与数据挖掘
date: 2021-10-18
categories: [大数据分析应用-中级]
tags: [大数据考试-计算机基础知识]
---


# 1、掌握感知机的定义和基本原理。
感知机是二分类的线性模型，其输入是实例的特征向量，输出的是事例的类别，分别是+1和-1，属于判别模型。

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面。如果是非线性可分的数据，则最后无法获得超平面。感知机由Rosenblatt于1957年提出的，是神经网络和支持向量机的基础。

![](/images/bigdata/4-9.png)
# 2、掌握 Logistic 回归算法原理与特点，能够使用 Logistic 回归进行数据的分类建模与参数解释。
Logistic回归虽然说是回归，但确是为了解决分类问题，是二分类任务的首选方法，简单来说，输出结果不是0就是1。

逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）。

逻辑回归假设因变量 y 服从二项分布，而线性回归假设因变量 y 服从高斯分布。

因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。

可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。
# 3、掌握朴素贝叶斯算法的定义与基本原理。
贝叶斯方法是以贝叶斯原理为基础，使用概率统计的知识对样本数据集进行分类。由于其有着坚实的数学基础，贝叶斯分类算法的误判率是很低的。贝叶斯方法的特点是结合先验概率和后验概率，即避免了只使用先验概率的主观偏见，也避免了单独使用样本信息的过拟合现象。贝叶斯分类算法在数据集较大的情况下表现出较高的准确率，同时算法本身也比较简单。

朴素贝叶斯分类（NBC）是以贝叶斯定理为基础并且假设特征条件之间相互独立的方法，先通过已给定的训练集，以特征词之间独立作为前提假设，学习从输入到输出的联合概率分布，再基于学习到的模型，输入 求出使得后验概率最大的输出 。

![](/images/bigdata/4-10.png)

# 4、掌握 k 近邻算法算法定义与原理。

# 5、掌握支持向量机（SVM）算法的思想与原理。

# 6、掌握决策树的算法的定义与原理，了解决策树的剪枝理论。

# 7、了解常见集成方法如 boosting、bagging 等。

# 8、掌握聚类分析的相关概念。

# 9、掌握主成分分析的原理。

# 10、了解生成式半监督学习方法、半监督 SVM、图半监督学习方法的思想与原理。