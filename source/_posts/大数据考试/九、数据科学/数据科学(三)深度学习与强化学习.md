---
title: 数据科学（三）深度学习与强化学习
date: 2021-10-18
categories: [大数据分析应用-中级]
tags: [大数据考试-计算机基础知识]
---


# 1、掌握全连接网络（MLP）的定义与算法原理、并能够应用。
全连接网络，是指前一层的每个神经元会都与下一层的全部神经元连接。
# 2、掌握卷积神经网络（CNN）基本概念，了解几种经典的卷积神经网络， 如 AlexNet、VGG、GoogLeNet、ResNet。
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）”。

AlexNet是2012年ILSVRC图像分类和物体识别算法的优胜者，也是LetNet-5之后受到人工智能领域关注的现代卷积神经网络算法。AlexNet的隐含层由5个卷积层、3个池化层和3个全连接层组成。

VGGNet是牛津大学视觉几何团队（Visual Geometry Group, VGG）开发的一组卷积神经网络算法，包括VGG-11、VGG-11-LRN、VGG-13、VGG-16和VGG-19。其中VGG-16是2014年ILSVRC物体识别算法的优胜者，其规模是AlexNet的2倍以上并拥有规律的结构，这里以VGG-16为例介绍其构筑。VGG-16的隐含层由13个卷积层、3个全连接层和5个池化层组成。

GoogLeNet是2014年ILSVRC图像分类算法的优胜者，是首个以Inception模块进行堆叠形成的大规模卷积神经网络。GoogLeNet共有四个版本：Inception v1、Inception v2、Inception v3、Inception v4。

ResNet来自微软的人工智能团队Microsoft Research，是2015年ILSVRC图像分类和物体识别算法的优胜者，其表现超过了GoogLeNet的第三代版本Inception v3。ResNet是使用残差块建立的大规模卷积神经网络，其规模是AlexNet的20倍、VGG-16的8倍，在ResNet的原始版本中，其残差块由2个卷积层、1个跳跃连接、BN和激励函数组成，ResNet的隐含层共包含16个残差块。
# 3、掌握循环神经网络（RNN）原理与基本概念，了解几种常见的模型，如 长短期记忆网络 LSTM、GRU。
循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。

对循环神经网络的研究始于二十世纪80-90年代，并在二十一世纪初发展为深度学习（deep learning）算法之一，其中双向循环神经网络（Bidirectional RNN, Bi-RNN）和长短期记忆网络（Long Short-Term Memory networks，LSTM）是常见的循环神经网络。

循环神经网络具有记忆性、参数共享并且图灵完备（Turing completeness），因此在对序列的非线性特征进行学习时具有一定优势。循环神经网络在自然语言处理（Natural Language Processing, NLP），例如语音识别、语言建模、机器翻译等领域有应用，也被用于各类时间序列预报。引入了卷积神经网络（Convolutional Neural Network,CNN）构筑的循环神经网络可以处理包含序列输入的计算机视觉问题。

LSTM是最早被提出的RNN门控算法，其对应的循环单元，LSTM单元包含3个门控：输入门、遗忘门和输出门。相对于RNN对系统状态建立的递归计算，3个门控对LSTM单元的内部状态建立了自循环（self-loop）。具体地，输入门决定当前时间步的输入和前一个时间步的系统状态对内部状态的更新；遗忘门决定前一个时间步内部状态对当前时间步内部状态的更新；输出门决定内部状态对系统状态的更新。

由于LSTM中3个门控对提升其学习能力的贡献不同，因此略去贡献小的门控和其对应的权重，可以简化神经网络结构并提升其学习效率。GRU即是根据以上观念提出的算法，其对应的循环单元仅包含2个门控：更新门和复位门，其中复位门的功能与LSTM单元的输入门相近，更新门则同时实现了遗忘门和输出门的功能。

# 4、掌握生成对抗网络（GAN、WGAN）的基本定义与原理，能够完成基本 应用
生成式对抗网络（GAN, Generative Adversarial Networks ）是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中（至少）两个模块：生成模型（Generative Model）和判别模型（Discriminative Model）的互相博弈学习产生相当好的输出。原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。


WGAN：
1、彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度
2、基本解决了collapse mode的问题，确保了生成样本的多样性
3、训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高
4、以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到

# 5、掌握马尔可夫决策过程的定义，了解其算法应用范围，并能够实际应用

马尔可夫决策过程（Markov Decision Process, MDP）是序贯决策（sequential decision）的数学模型，用于在系统状态具有马尔可夫性质的环境中模拟智能体可实现的随机性策略与回报。MDP的得名来自于俄国数学家安德雷·马尔可夫（Андрей Андреевич Марков），以纪念其为马尔可夫链所做的研究。

MDP基于一组交互对象，即智能体和环境进行构建，所具有的要素包括状态、动作、策略和奖励。在MDP的模拟中，智能体会感知当前的系统状态，按策略对环境实施动作，从而改变环境的状态并得到奖励，奖励随时间的积累被称为回报。

MDP的理论基础是马尔可夫链，因此也被视为考虑了动作的马尔可夫模型。在离散时间上建立的MDP被称为“离散时间马尔可夫决策过程（descrete-time MDP）”，反之则被称为“连续时间马尔可夫决策过程（continuous-time MDP）”。此外MDP存在一些变体，包括部分可观察马尔可夫决策过程、约束马尔可夫决策过程和模糊马尔可夫决策过程。

在应用方面，MDP被用于机器学习中强化学习（reinforcement learning）问题的建模。通过使用动态规划、随机采样等方法，MDP可以求解使回报最大化的智能体策略，并在自动控制、推荐系统等主题中得到应用。

# 6、掌握经典强化学习算法（Q-learning、SARMA）的定义与原理。
强化学习的概念，通俗的讲，强化学习就是通过agent，也就是动作的发起者，对环境造成一个影响，环境接受该动作后状态发生变化，同时产生一个强化信号(奖或惩)反馈给Agent，Agent根据强化信号和环境当前状态再选择下一个动作，选择的原则是使受到正强化(奖)的概率增大。选择的动作不仅影响立即强化值，而且影响环境下一时刻的状态及最终的强化值。在强化学习中，包含两种基本的元素：状态与动作，在某个状态下执行某种动作，这便是一种策略，学习器要做的就是通过不断地探索学习，从而获得一个好的策略。

Q-Learning属于强化学习的经典算法，用于解决马尔可夫决策问题。为无监督学习。
# 7、掌握经典深度强化学习算法（DQN、DDPG、A3C、TRPO、PPO）的定义 与原理，了解其算法特性。


DeepMind在2013年提出的DQN算法（2015年提出了DQN的改进版本）可以说是深度学习和强化学习的第一次成功结合。要想将深度学习融合进强化学习，是有一些很关键的问题需要解决的，其中的两个问题如下：
1、深度学习需要大量有标签的数据样本；而强化学习是智能体主动获取样本，样本量稀疏且有延迟。
2、深度学习要求每个样本相互之间是独立同分布的；而强化学习获取的相邻样本相互关联，并不是相互独立的。


DQN是一种基于值函数的方法，基于值函数的方法难以应对的是大的动作空间，特别是连续动作情况。因为网络难以有这么多输出，且难以在这么多输出之中搜索最大的Q值。而DDPG是基于上面所讲到的Actor-Critic方法，在动作输出方面采用一个网络来拟合策略函数，直接输出动作，可以应对连续动作的输出及大的动作空间。

